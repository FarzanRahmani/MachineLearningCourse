{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ea97689d8f824f7a947b70751b93dbb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94e31db2368a4d548954d1793f30e589","IPY_MODEL_2eb227132aa74f55bb6899329529771b","IPY_MODEL_09a0e6ef2d8549598d22717d06b75772"],"layout":"IPY_MODEL_7919d4f46f34495894693ac7e8bd1192"}},"94e31db2368a4d548954d1793f30e589":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c54d1926940493b9aeb3cd490da9682","placeholder":"​","style":"IPY_MODEL_0b4e610b2cc64aa796d2afac1e61866c","value":"tokenizer_config.json: 100%"}},"2eb227132aa74f55bb6899329529771b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffde291413ab49679d2267a908dab8c9","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8af2085455c487ba0361b227df4dd91","value":48}},"09a0e6ef2d8549598d22717d06b75772":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04715d728f004251bbdef5cd2832a245","placeholder":"​","style":"IPY_MODEL_d9131c294da240d5aead8949add8c8fa","value":" 48.0/48.0 [00:00&lt;00:00, 2.30kB/s]"}},"7919d4f46f34495894693ac7e8bd1192":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c54d1926940493b9aeb3cd490da9682":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b4e610b2cc64aa796d2afac1e61866c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffde291413ab49679d2267a908dab8c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8af2085455c487ba0361b227df4dd91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04715d728f004251bbdef5cd2832a245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9131c294da240d5aead8949add8c8fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f1d069a3c2a48ab94981d7a01b27bc3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84102f43c5014323ad908c87c656b099","IPY_MODEL_992a7fcaba774db898bea350ad055eac","IPY_MODEL_520103989b144fafb68872b56d94d939"],"layout":"IPY_MODEL_d350698f76b04fd0ab15e05dafbeaeaf"}},"84102f43c5014323ad908c87c656b099":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b806dd4b2dce4d67b8bcede59ef937d5","placeholder":"​","style":"IPY_MODEL_b57d9e96db624f4eb43f2d9e9db01220","value":"vocab.txt: 100%"}},"992a7fcaba774db898bea350ad055eac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_91a26d65822844b881cbd023ab824d88","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7581707b4d7e4e5a833e9d89a0c2d66e","value":231508}},"520103989b144fafb68872b56d94d939":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_195d36c5a9344f67aff7ff96f0356ae2","placeholder":"​","style":"IPY_MODEL_2af49d6afc3e4fbfad7b1a0ab8257a4a","value":" 232k/232k [00:00&lt;00:00, 8.71MB/s]"}},"d350698f76b04fd0ab15e05dafbeaeaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b806dd4b2dce4d67b8bcede59ef937d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b57d9e96db624f4eb43f2d9e9db01220":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91a26d65822844b881cbd023ab824d88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7581707b4d7e4e5a833e9d89a0c2d66e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"195d36c5a9344f67aff7ff96f0356ae2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2af49d6afc3e4fbfad7b1a0ab8257a4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7882fed4a0f4bfd9f0a6a6372367eda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ebaa283bdc21477a9b6801a9d8099fb7","IPY_MODEL_31a1e0fe0602407a9042851f35116f57","IPY_MODEL_0ca188a34d8e49b9980e98864d9a1013"],"layout":"IPY_MODEL_383b400ac6d148308d846541dee345a6"}},"ebaa283bdc21477a9b6801a9d8099fb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e145b355d324431fa4349adddf56b4d7","placeholder":"​","style":"IPY_MODEL_5e262d97a0674ca9a42e9d23ddaee3c5","value":"tokenizer.json: 100%"}},"31a1e0fe0602407a9042851f35116f57":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca690430311a4f08949a15c5996cc4c7","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc764acda441431d9d1bc28394b50942","value":466062}},"0ca188a34d8e49b9980e98864d9a1013":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f35169ae6692462e973ba78bdc17e4d8","placeholder":"​","style":"IPY_MODEL_577c5fd437f54a959d26789b79659a4d","value":" 466k/466k [00:00&lt;00:00, 1.10MB/s]"}},"383b400ac6d148308d846541dee345a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e145b355d324431fa4349adddf56b4d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e262d97a0674ca9a42e9d23ddaee3c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca690430311a4f08949a15c5996cc4c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc764acda441431d9d1bc28394b50942":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f35169ae6692462e973ba78bdc17e4d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"577c5fd437f54a959d26789b79659a4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2493e81822cc4b9788be188f1e0611d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6f143928c9040dfabf0a7a04a494fbf","IPY_MODEL_2f42361fdd2b45cfabcb4cdcdaf71eae","IPY_MODEL_c31fa137ae9a431098d53a04818d4680"],"layout":"IPY_MODEL_448a64809fef4d7db608673da60afd7d"}},"d6f143928c9040dfabf0a7a04a494fbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f16e3dc4c4f04671b107d27644362932","placeholder":"​","style":"IPY_MODEL_d50dad19b3c54129bbb63082059b4986","value":"config.json: 100%"}},"2f42361fdd2b45cfabcb4cdcdaf71eae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b09d6324b1f047848eba4a88b3cddcf9","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_007b93cae166449d9480d678619e4010","value":570}},"c31fa137ae9a431098d53a04818d4680":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_805e969ea6b64dc88233c240dc410656","placeholder":"​","style":"IPY_MODEL_a5a5fdd1b6044af8be34cc47a2e69dd0","value":" 570/570 [00:00&lt;00:00, 39.7kB/s]"}},"448a64809fef4d7db608673da60afd7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f16e3dc4c4f04671b107d27644362932":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d50dad19b3c54129bbb63082059b4986":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b09d6324b1f047848eba4a88b3cddcf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"007b93cae166449d9480d678619e4010":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"805e969ea6b64dc88233c240dc410656":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5a5fdd1b6044af8be34cc47a2e69dd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<font>\n<div dir=ltr align=center>\n<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n<font color=0F5298 size=7>\n    Machine learning <br>\n<font color=2565AE size=5>\n    Computer Engineering Department <br>\n    Fall 2024<br>\n<font color=3C99D size=5>\n    Practical Assignment 5 - NLP - Transformer & Bert <br>\n</div>\n<div dir=ltr align=center>\n<font color=0CBCDF size=4>\n   &#x1F349; Masoud Tahmasbi  &#x1F349;  &#x1F353; Arash Ziyaei &#x1F353;\n<br>\n<font color=0CBCDF size=4>\n   &#x1F335; Amirhossein Akbari  &#x1F335;\n</div>\n\n____","metadata":{"id":"VivaFsd3Q6cj"}},{"cell_type":"markdown","source":"<font color=9999FF size=4>\n&#x1F388; Full Name : Farzan Rahmani\n<br>\n<font color=9999FF size=4>\n&#x1F388; Student Number : 403210725","metadata":{"id":"k15QziPnmC6d"}},{"cell_type":"markdown","source":"<font color=0080FF size=3>\nThis notebook covers two key topics. First, we implement a transformer model from scratch and apply it to a specific task. Second, we fine-tune the BERT model using LoRA for efficient adaptation to a downstream task.\n</font>\n<br>\n\n**Note:**\n<br>\n<font color=66B2FF size=2>In this notebook, you are free to use any function or model from PyTorch to assist with the implementation. However, TensorFlow is not permitted for this exercise. This ensures consistency and alignment with the tools being focused on.</font>\n<br>\n<font color=red size=3>**Run All Cells Before Submission**</font>: <font color=FF99CC size=2>Before saving and submitting your notebook, please ensure you run all cells from start to finish. This practice guarantees that your notebook is self-consistent and can be evaluated correctly by others.</font>","metadata":{"id":"IOfpEN2xmbN8"}},{"cell_type":"markdown","source":"# Section 1: Transformer\n\nThe transformer architecture consists of two main components: an encoder and a decoder. Each of these components is made up of multiple layers that include self-attention mechanisms and feedforward neural networks. The self-attention mechanism is central to the transformer, as it enables the model to assess the importance of different words in a sentence by considering their relationships with one another.\n\n\nIn this assignment, you should design a transformer model from scratch. You are required to implement the Encoder and Decoder components of a Transformer model.","metadata":{"id":"SpvvM0995ieR"}},{"cell_type":"code","source":"!pip install datasets -q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HsM8BZ4jh57","outputId":"29999cab-c7fa-4200-883a-ae1e1bb96598","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:18:58.063614Z","iopub.execute_input":"2025-01-09T22:18:58.063893Z","iopub.status.idle":"2025-01-09T22:19:02.679611Z","shell.execute_reply.started":"2025-01-09T22:18:58.063871Z","shell.execute_reply":"2025-01-09T22:19:02.678409Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Importing libraries\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\n\n# Math\nimport math\n\n# HuggingFace libraries\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Pathlib\nfrom pathlib import Path\n\n# typing\nfrom typing import Any\n\n# Library for progress bars in loops\nfrom tqdm import tqdm\n\n# Importing library of warnings\nimport warnings","metadata":{"id":"dzIob6-Gq7Lw","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:19:02.680966Z","iopub.execute_input":"2025-01-09T22:19:02.681317Z","iopub.status.idle":"2025-01-09T22:19:13.372928Z","shell.execute_reply.started":"2025-01-09T22:19:02.681279Z","shell.execute_reply":"2025-01-09T22:19:13.371957Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Part 1: Input Embeddings\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we observe the Transformer architecture image above, we can see that the Embeddings represent the first step of both blocks.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>InputEmbedding</code> class below is responsible for converting the input text into numerical vectors of <code>d_model</code> dimensions. To prevent that our input embeddings become extremely small, we normalize them by multiplying them by the $\\sqrt{d_{model}}$.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the image below, we can see how the embeddings are created. First, we have a sentence that gets split into tokens—we will explore what tokens are later on—. Then, the token IDs—identification numbers—are transformed into the embeddings, which are high-dimensional vectors.</p>","metadata":{"id":"-71SfIAJprox"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a class `InputEmbeddings` inheriting from `nn.Module`\n# - Initialize the class with two parameters:\n#   1. `d_model`: Dimension of the embedding vectors\n#   2. `vocab_size`: Size of the vocabulary\n# - Create an embedding layer using `nn.Embedding` to map input indices to dense vectors\n\n# - In the `forward` method:\n#   1. Pass the input `x` through the embedding layer\n#   2. Scale the embeddings by the square root of `d_model` for variance normalization\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass InputEmbeddings(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        # (batch_size, seq_len) --> (batch_size, seq_len, d_model)\n        return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"id":"J-pyrJlu4Nl7","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:19:13.374415Z","iopub.execute_input":"2025-01-09T22:19:13.374970Z","iopub.status.idle":"2025-01-09T22:19:13.379720Z","shell.execute_reply.started":"2025-01-09T22:19:13.374944Z","shell.execute_reply":"2025-01-09T22:19:13.378752Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Part 2: positional encoding\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the authors add the positional encodings to the input embeddings at the bottom of both the encoder and decoder blocks so the model can have some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two vectors can be summed and we can combine the semantic content from the word embeddings and positional information from the positional encodings.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>PositionalEncoding</code> class below, we will create a matrix of positional encodings <code>pe</code> with dimensions <code>(seq_len, d_model)</code>. We will start by filling it with $0$s.We will then apply the sine function to even indices of the positional encoding matrix while the cosine function is applied to the odd ones.</p>\n\n<p style=\"\n    margin-bottom: 5;\n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000;\n  \">\n    \\begin{equation}\n    \\text{Odd Indices } (2i + 1): \\quad \\text{PE(pos, } 2i + 1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n    \\end{equation}\n</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We apply the sine and cosine functions because it allows the model to determine the position of a word based on the position of other words in the sequence, since for any fixed offset $k$, $PE_{pos + k}$ can be represented as a linear function of $PE_{pos}$. This happens due to the properties of sine and cosine functions, where a shift in the input results in a predictable change in the output.</p>","metadata":{"id":"RWBlo2XorJGW"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `PositionalEncoding` class inheriting from `nn.Module`\n# - Initialize with `d_model`, `seq_len`, and `dropout`\n# - Generate a positional encoding matrix using sine and cosine functions\n# - Register the positional encoding as a non-trainable buffer\n# - In `forward`, add positional encoding to input and apply dropout\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, seq_len: int, dropout: float):\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n\n        # Create a matrix of shape (seq_len, d_model)\n        pe = torch.zeros(seq_len, d_model)\n        # Create a vector of shape (seq_len, 1)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        # Apply sine to even indices\n        pe[:, 0::2] = torch.sin(position * div_term)\n        # Apply cosine to odd indices\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # Add a batch dimension to the positional encoding\n        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n        # Register the positional encoding as a buffer\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n        return self.dropout(x)","metadata":{"id":"4ZG5DhVcrVVm","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:19:13.380956Z","iopub.execute_input":"2025-01-09T22:19:13.381308Z","iopub.status.idle":"2025-01-09T22:19:13.409661Z","shell.execute_reply.started":"2025-01-09T22:19:13.381284Z","shell.execute_reply":"2025-01-09T22:19:13.408793Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Part 3: layer normalization\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the encoder and decoder blocks, we see several normalization layers called <b><i>Add &amp; Norm</i></b>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>LayerNormalization</code> class below performs layer normalization on the input data. During its forward pass, we compute the mean and standard deviation of the input data. We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called epsilon to avoid any divisions by zero. This process results in a normalized output with a mean 0 and a standard deviation 1.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will then scale the normalized output by a learnable parameter <code>alpha</code> and add a learnable parameter called <code>bias</code>. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent.</p>","metadata":{"id":"T92iydQErh-P"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `LayerNormalization` class inheriting from `nn.Module`\n# - Initialize with `eps` (small value to prevent division by zero)\n# - Define trainable parameters:\n#   1. `alpha`: Scaling factor initialized to 1\n#   2. `bias`: Offset initialized to 0\n\n# - In `forward`, perform layer normalization:\n#   1. Compute mean and standard deviation along the last dimension\n#   2. Normalize the input using the computed mean and std\n#   3. Scale and shift using `alpha` and `bias`\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass LayerNormalization(nn.Module):\n    def __init__(self, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(1))  # Multiplicative factor\n        self.bias = nn.Parameter(torch.zeros(1))  # Additive factor\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias","metadata":{"id":"kVGQRsmKrwZu","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:19:20.910954Z","iopub.execute_input":"2025-01-09T22:19:20.911312Z","iopub.status.idle":"2025-01-09T22:19:20.916768Z","shell.execute_reply.started":"2025-01-09T22:19:20.911275Z","shell.execute_reply":"2025-01-09T22:19:20.915795Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Part 4: Feed Forward Network\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the fully connected feed-forward network, we apply two linear transformations with a ReLU activation in between. We can mathematically represent this operation as:</p>\n\n<p style=\"\n    margin-bottom: 5;\n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000;\n  \">\n    \\begin{equation}\n    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n    \\end{equation}\n</p>\n\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">$W_1$ and $W_2$ are the weights, while $b_1$ and $b_2$ are the biases of the two linear transformations.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>FeedForwardBlock</code> below, we will define the two linear transformations—<code>self.linear_1</code> and <code>self.linear_2</code>—and the inner-layer <code>d_ff</code>. The input data will first pass through the <code>self.linear_1</code> transformation, which increases its dimensionality from <code>d_model</code> to <code>d_ff</code>. The output of this operation passes through the ReLU activation function, which introduces non-linearity so the network can learn more complex patterns, and the <code>self.dropout</code> layer is applied to mitigate overfitting. The final operation is the <code>self.linear_2</code> transformation to the dropout-modified tensor, which transforms it back to the original <code>d_model</code> dimension.</p>","metadata":{"id":"U-IbSGQMr1Ye"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `FeedForwardBlock` class inheriting from `nn.Module`\n# - Initialize with `d_model`, `d_ff`, and `dropout`\n# - Define:\n#   1. `linear_1`: Linear layer projecting from `d_model` to `d_ff`\n#   2. Dropout layer for regularization\n#   3. `linear_2`: Linear layer projecting back from `d_ff` to `d_model`\n\n# - In `forward`, apply the following steps:\n#   1. Pass input through `linear_1` followed by ReLU activation\n#   2. Apply dropout\n#   3. Pass through `linear_2` to return to original dimensions\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass FeedForwardBlock(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout: float):\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)  # W1 and b1\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)  # W2 and b2\n\n    def forward(self, x):\n        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_ff) --> (batch_size, seq_len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))","metadata":{"id":"N3H8kyccsEUW","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:19:31.237982Z","iopub.execute_input":"2025-01-09T22:19:31.238292Z","iopub.status.idle":"2025-01-09T22:19:31.243279Z","shell.execute_reply.started":"2025-01-09T22:19:31.238266Z","shell.execute_reply":"2025-01-09T22:19:31.242405Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Part 5: Multi Head Attention\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention is the most crucial component of the Transformer. It is responsible for helping the model to understand complex relationships and patterns in the data.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The image below displays how the Multi-Head Attention works. It doesn't include <code>batch</code> dimension because it only illustrates the process for one single sentence.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention block receives the input data split into queries, keys, and values organized into matrices $Q$, $K$, and $V$. Each matrix contains different facets of the input, and they have the same dimensions as the input.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We then linearly transform each matrix by their respective weight matrices $W^Q$, $W^K$, and $W^V$. These transformations will result in new matrices $Q'$, $K'$, and $V'$, which will be split into smaller matrices corresponding to different heads $h$, allowing the model to attend to information from different representation subspaces in parallel. This split creates multiple sets of queries, keys, and values for each head.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Finally, we concatenate every head into an $H$ matrix, which is then transformed by another weight matrix $W^o$ to produce the multi-head attention output, a matrix $MH-A$ that retains the input dimensionality.</p>","metadata":{"id":"YEa1kF6csIvV"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `MultiHeadAttentionBlock` class inheriting from `nn.Module`\n# - Initialize with `d_model` (model dimensions), `h` (number of heads), and `dropout`:\n#   1. Assert `d_model` is divisible by `h`\n#   2. Define `d_k` as dimensions per head\n#   3. Create weight matrices (`w_q`, `w_k`, `w_v`, `w_o`) for query, key, value, and output\n#   4. Add a dropout layer for regularization\n\n# - Implement a static `attention` method to:\n#   1. Compute scaled dot-product attention\n#   2. Apply mask if provided\n#   3. Apply softmax and dropout\n#   4. Return weighted values and attention scores\n\n# - In `forward`, perform:\n#   1. Linear transformation of input into query, key, and value\n#   2. Split into `h` heads and rearrange dimensions\n#   3. Compute attention output and scores using `attention`\n#   4. Combine heads and apply output weight matrix\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass MultiHeadAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, h: int, dropout: float):\n        super().__init__()\n        self.d_model = d_model\n        self.h = h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n\n        self.d_k = d_model // h\n        self.w_q = nn.Linear(d_model, d_model)  # Wq\n        self.w_k = nn.Linear(d_model, d_model)  # Wk\n        self.w_v = nn.Linear(d_model, d_model)  # Wv\n        self.w_o = nn.Linear(d_model, d_model)  # Wo\n        self.dropout = nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout):\n        d_k = query.shape[-1]\n        # (batch_size, h, seq_len, d_k) --> (batch_size, h, seq_len, seq_len)\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            attention_scores.masked_fill_(mask == 0, -1e9)\n        attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len, seq_len)\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        return (attention_scores @ value), attention_scores\n\n    def forward(self, q, k, v, mask):\n        query = self.w_q(q)  # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_model)\n        key = self.w_k(k)  # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_model)\n        value = self.w_v(v)  # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_model)\n\n        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, h, d_k) --> (batch_size, h, seq_len, d_k)\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n\n        # (batch_size, h, seq_len, d_k) --> (batch_size, seq_len, h, d_k) --> (batch_size, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_model)\n        return self.w_o(x)","metadata":{"id":"6ujcqPp1sOU9","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:20:26.947803Z","iopub.execute_input":"2025-01-09T22:20:26.948132Z","iopub.status.idle":"2025-01-09T22:20:26.956872Z","shell.execute_reply.started":"2025-01-09T22:20:26.948094Z","shell.execute_reply":"2025-01-09T22:20:26.955795Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Part 6: Residual Connection\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the architecture of the Transformer, we see that each sub-layer, including the <i>self-attention</i> and <i>Feed Forward</i> blocks, adds its output to its input before passing it to the <i>Add &amp; Norm</i> layer. This approach integrates the output with the original input in the <i>Add &amp; Norm</i> layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>ResidualConnection</code> class below is responsible for this process.</p>","metadata":{"id":"wCaLjCVxsWIc"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `ResidualConnection` class inheriting from `nn.Module`\n# - Initialize with `dropout`:\n#   1. Add a dropout layer for regularization\n#   2. Include a layer normalization instance\n\n# - In `forward`:\n#   1. Normalize the input using the normalization layer\n#   2. Pass the normalized input through the sublayer\n#   3. Apply dropout and add the result back to the original input for residual connection\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass ResidualConnection(nn.Module):\n    def __init__(self, dropout: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalization()\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"id":"f-bvuGhIsdfu","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:20:31.673894Z","iopub.execute_input":"2025-01-09T22:20:31.674255Z","iopub.status.idle":"2025-01-09T22:20:31.679067Z","shell.execute_reply.started":"2025-01-09T22:20:31.674220Z","shell.execute_reply":"2025-01-09T22:20:31.678102Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Part 7: Encoder\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now build the encoder. We create the <code>EncoderBlock</code> class, consisting of the Multi-Head Attention and Feed Forward layers, plus the residual connections.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the Encoder Block repeats six times. We create the <code>Encoder</code> class as an assembly of multiple <code>EncoderBlock</code>s. We also add layer normalization as a final step after processing the input through all its blocks.</p>","metadata":{"id":"9YYI5vpasdGm"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create an `EncoderBlock` class inheriting from `nn.Module`\n# - Initialize with:\n#   1. `self_attention_block`: Multi-head attention block\n#   2. `feed_forward_block`: Feed-forward block\n#   3. `dropout`: Dropout rate for residual connections\n# - Define two residual connections for:\n#   1. Self-attention block\n#   2. Feed-forward block\n\n# - In `forward`:\n#   1. Apply the first residual connection with the self-attention block\n#   2. Apply the second residual connection with the feed-forward block\n#   3. Return the updated tensor after both layers\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass EncoderBlock(nn.Module):\n    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n\n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x","metadata":{"id":"fRtppwE1s0-t","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:20:42.266905Z","iopub.execute_input":"2025-01-09T22:20:42.267289Z","iopub.status.idle":"2025-01-09T22:20:42.272850Z","shell.execute_reply.started":"2025-01-09T22:20:42.267256Z","shell.execute_reply":"2025-01-09T22:20:42.271774Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create an `Encoder` class inheriting from `nn.Module`\n# - Initialize with:\n#   1. `layers`: A list of `EncoderBlock` instances\n#   2. A layer normalization instance for output normalization\n\n# - In `forward`:\n#   1. Pass the input tensor `x` through each `EncoderBlock` in `self.layers`\n#   2. Apply the mask during each block's forward pass\n#   3. Normalize the final output and return it\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass Encoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"id":"eSq7BZWcs5s1","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:20:49.825924Z","iopub.execute_input":"2025-01-09T22:20:49.826283Z","iopub.status.idle":"2025-01-09T22:20:49.831048Z","shell.execute_reply.started":"2025-01-09T22:20:49.826251Z","shell.execute_reply":"2025-01-09T22:20:49.830184Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Part 8: Decoder\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Similarly, the Decoder also consists of several DecoderBlocks that repeat six times in the original paper. The main difference is that it has an additional sub-layer that performs multi-head attention with a <i>cross-attention</i> component that uses the output of the Encoder as its keys and values while using the Decoder's input as queries.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For the Output Embedding, we can use the same <code>InputEmbeddings</code> class we use for the Encoder. You can also notice that the self-attention sub-layer is <i>masked</i>, which restricts the model from accessing future elements in the sequence.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will start by building the <code>DecoderBlock</code> class, and then we will build the <code>Decoder</code> class, which will assemble multiple <code>DecoderBlock</code>s.</p>","metadata":{"id":"P0HXE1fH5g0W"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `DecoderBlock` class inheriting from `nn.Module`\n# - Initialize with:\n#   1. `self_attention_block`: Multi-head self-attention block\n#   2. `cross_attention_block`: Multi-head cross-attention block\n#   3. `feed_forward_block`: Feed-forward block\n#   4. `dropout`: Dropout rate\n# - Define three residual connections for:\n#   1. Self-attention block\n#   2. Cross-attention block\n#   3. Feed-forward block\n\n# - In `forward`:\n#   1. Apply the self-attention block with target mask and residual connection\n#   2. Apply the cross-attention block with source mask and residual connection\n#   3. Apply the feed-forward block with residual connection\n#   4. Return the updated tensor\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass DecoderBlock(nn.Module):\n    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x","metadata":{"id":"V9Aof9mb4PJX","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:21:01.541018Z","iopub.execute_input":"2025-01-09T22:21:01.541360Z","iopub.status.idle":"2025-01-09T22:21:01.547350Z","shell.execute_reply.started":"2025-01-09T22:21:01.541329Z","shell.execute_reply":"2025-01-09T22:21:01.546272Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `Decoder` class inheriting from `nn.Module`\n# - Initialize with:\n#   1. `layers`: A list of `DecoderBlock` instances\n#   2. A layer normalization instance for the final output\n\n# - In `forward`:\n#   1. Pass the input tensor `x` through each `DecoderBlock` in `self.layers`\n#   2. Provide `encoder_output`, `src_mask`, and `tgt_mask` to each block\n#   3. Normalize the final output using the layer normalization\n#   4. Return the normalized output\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass Decoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return self.norm(x)","metadata":{"id":"vwdthvkrtNUM","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:21:05.701080Z","iopub.execute_input":"2025-01-09T22:21:05.701449Z","iopub.status.idle":"2025-01-09T22:21:05.706192Z","shell.execute_reply.started":"2025-01-09T22:21:05.701422Z","shell.execute_reply":"2025-01-09T22:21:05.705234Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">You can see in the Decoder image that after running a stack of <code>DecoderBlock</code>s, we have a Linear Layer and a Softmax function to the output of probabilities. The <code>ProjectionLayer</code> class below is responsible for converting the output of the model into a probability distribution over the <i>vocabulary</i>, where we select each output token from a vocabulary of possible tokens.</p>","metadata":{"id":"Qm4g_8O1tS3d"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `ProjectionLayer` class inheriting from `nn.Module`\n# - Initialize with:\n#   1. `d_model`: Dimension of the model\n#   2. `vocab_size`: Size of the output vocabulary\n# - Define a linear layer to project from `d_model` to `vocab_size`\n\n# - In `forward`:\n#   1. Pass the input through the linear layer\n#   2. Apply log Softmax along the last dimension\n#   3. Return the log probabilities\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, vocab_size)\n        return torch.log_softmax(self.proj(x), dim=-1)","metadata":{"id":"UbWVoNintThN","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:21:13.932876Z","iopub.execute_input":"2025-01-09T22:21:13.933237Z","iopub.status.idle":"2025-01-09T22:21:13.938140Z","shell.execute_reply.started":"2025-01-09T22:21:13.933200Z","shell.execute_reply":"2025-01-09T22:21:13.937080Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Part 9: Building the Transformer\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally have every component of the Transformer architecture ready. We may now construct the Transformer by putting it all together.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>Transformer</code> class below, we will bring together all the components of the model's architecture.</p>","metadata":{"id":"waCzPEAxtaR8"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Create a `Transformer` class inheriting from `nn.Module`\n# - Initialize with:\n#   1. `encoder`: Encoder module\n#   2. `decoder`: Decoder module\n#   3. `src_embed` and `tgt_embed`: Input embeddings for source and target languages\n#   4. `src_pos` and `tgt_pos`: Positional encodings for source and target languages\n#   5. `projection_layer`: Linear projection layer for final output\n\n# - Define the `encode` method:\n#   1. Apply source embeddings to input\n#   2. Add positional encoding\n#   3. Pass through the encoder with the source mask\n#   4. Return the encoded representation\n\n# - Define the `decode` method:\n#   1. Apply target embeddings to input\n#   2. Add positional encoding\n#   3. Pass through the decoder with encoder output, source mask, and target mask\n#   4. Return the decoder's output\n\n# - Define the `project` method:\n#   1. Pass decoder output through the projection layer\n#   2. Apply log Softmax to obtain probabilities\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass Transformer(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n\n    def encode(self, src, src_mask):\n        src = self.src_embed(src)\n        src = self.src_pos(src)\n        return self.encoder(src, src_mask)\n\n    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n        tgt = self.tgt_embed(tgt)\n        tgt = self.tgt_pos(tgt)\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n\n    def project(self, x):\n        return self.projection_layer(x)","metadata":{"id":"qXbPW4oCtk2G","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:21:26.124341Z","iopub.execute_input":"2025-01-09T22:21:26.124636Z","iopub.status.idle":"2025-01-09T22:21:26.131318Z","shell.execute_reply.started":"2025-01-09T22:21:26.124613Z","shell.execute_reply":"2025-01-09T22:21:26.130166Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The architecture is finally ready. We now define a function called <code>build_transformer</code>, in which we define the parameters and everything we need to have a fully operational Transformer model for the task of <b>machine translation</b>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will set the same parameters as in the original paper, <a href = \"https://arxiv.org/pdf/1706.03762.pdf\"><i>Attention Is All You Need</i></a>, where $d_{model}$ = 512, $N$ = 6, $h$ = 8, dropout rate $P_{drop}$ = 0.1, and $d_{ff}$ = 2048.</p>","metadata":{"id":"6znypMaetmRk"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `build_transformer` function with parameters for:\n#   1. Vocabulary sizes (`src_vocab_size`, `tgt_vocab_size`)\n#   2. Sequence lengths (`src_seq_len`, `tgt_seq_len`)\n#   3. Model dimensions (`d_model`, `d_ff`)\n#   4. Number of layers (`N`) and heads (`h`)\n#   5. Dropout rate (`dropout`)\n\n# - Create:\n#   1. Source and target embedding layers\n#   2. Positional encoding layers for source and target\n#   3. Encoder blocks with self-attention and feed-forward layers\n#   4. Decoder blocks with self-attention, cross-attention, and feed-forward layers\n#   5. Encoder and Decoder modules using the blocks\n#   6. Projection layer to map decoder output to target vocabulary\n\n# - Assemble all components into a `Transformer` instance\n# - Initialize parameters with Xavier uniform initialization\n# - Return the initialized Transformer\n\n######################  TODO  ########################\n######################  TODO  ########################\n\ndef build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n    # Create the embedding layers\n    src_embed = InputEmbeddings(d_model, src_vocab_size)\n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n\n    # Create the positional encoding layers\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n\n    # Create the encoder blocks\n    encoder_blocks = []\n    for _ in range(N):\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    # Create the decoder blocks\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n\n    # Create the encoder and decoder\n    encoder = Encoder(nn.ModuleList(encoder_blocks))\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n\n    # Create the projection layer\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n\n    # Create the transformer\n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n\n    # Initialize the parameters\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n\n    return transformer","metadata":{"id":"bqGnJ6w2twJc","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:21:32.109711Z","iopub.execute_input":"2025-01-09T22:21:32.110003Z","iopub.status.idle":"2025-01-09T22:21:32.117379Z","shell.execute_reply.started":"2025-01-09T22:21:32.109980Z","shell.execute_reply":"2025-01-09T22:21:32.116248Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"The model is now ready to be trained!","metadata":{"id":"Iw7CWf4bt3yr"}},{"cell_type":"markdown","source":"## Part 10: Tokenizer","metadata":{"id":"6_7Z3fEYuTK0"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Tokenization is a crucial preprocessing step for our Transformer model. In this step, we convert raw text into a number format that the model can process.  </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">There are several Tokenization strategies. We will use the <i>word-level tokenization</i> to transform each word in a sentence into a token.</p>","metadata":{"id":"EDinqTghqr_Q"}},{"cell_type":"markdown","source":"<center>\n    <img src = \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5e749c-b0bd-4496-85a1-9b4397ad935f_1400x787.jpeg\" width = 800, height= 800>\n<p style = \"font-size: 16px;\n            font-family: 'Georgia', serif;\n            text-align: center;\n            margin-top: 10px;\">Different tokenization strategies. Source: <a href = \"https://shaankhosla.substack.com/p/talking-tokenization\">shaankhosla.substack.com</a>.</p>\n</center>","metadata":{"id":"at-cYYjnqr_Q"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">After tokenizing a sentence, we map each token to an unique integer ID based on the created vocabulary present in the training corpus during the training of the tokenizer. Each integer number represents a specific word in the vocabulary.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Besides the words in the training corpus, Transformers use special tokens for specific purposes. These are some that we will define right away:</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>• [UNK]:</b> This token is used to identify an unknown word in the sequence.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>• [PAD]:</b> Padding token to ensure that all sequences in a batch have the same length, so we pad shorter sentences with this token. We use attention masks to <i>\"tell\"</i> the model to ignore the padded tokens during training since they don't have any real meaning to the task.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>•  [SOS]:</b> This is a token used to signal the <i>Start of Sentence</i>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>•  [EOS]:</b> This is a token used to signal the <i>End of Sentence</i>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>build_tokenizer</code> function below, we ensure a tokenizer is ready to train the model. It checks if there is an existing tokenizer, and if that is not the case, it trains a new tokenizer.</p>","metadata":{"id":"gjRMr2N6qr_Q"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `build_tokenizer` function with parameters for:\n#   1. `config`: Configuration containing tokenizer file path\n#   2. `ds`: Dataset to train the tokenizer\n#   3. `lang`: Language for which the tokenizer is built\n\n# - Check if the tokenizer file exists:\n#   1. If not, create a new tokenizer:\n#      - Initialize a word-level tokenizer with an unknown token (`[UNK]`)\n#      - Set the pre-tokenizer to split text by whitespace\n#      - Define a trainer with special tokens and minimum frequency\n#      - Train the tokenizer on all sentences in the dataset\n#      - Save the trained tokenizer to the specified file path\n#   2. If the file exists, load the tokenizer from the file\n\n# - Return the loaded or trained tokenizer\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not tokenizer_path.exists():\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer","metadata":{"id":"Zh9pOItduxHq","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:21:42.859136Z","iopub.execute_input":"2025-01-09T22:21:42.859478Z","iopub.status.idle":"2025-01-09T22:21:42.864323Z","shell.execute_reply.started":"2025-01-09T22:21:42.859456Z","shell.execute_reply":"2025-01-09T22:21:42.863361Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Part 11: Load Dataset","metadata":{"id":"oodlr4eouxTU"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For this task, we will use the <a href = \"opus_books · Datasets at Hugging Face\">OpusBooks dataset</a>, available on 🤗Hugging Face. This dataset consists of two features, <code>id</code> and <code>translation</code>. The <code>translation</code> feature contains pairs of sentences in different languages, such as Spanish and Portuguese, English and French, and so forth.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I first tried translating sentences from English to Portuguese—my native tongue — but there are only 1.4k examples for this pair, so the results were not satisfying in the current configurations for this model. I then tried to use the English-French pair due to its higher number of examples—127k—but it would take too long to train with the current configurations. I then opted to train the model on the English-Italian pair, the same one used in the <a href = \"https://youtu.be/ISNdQcPhsts?si=253J39cose6IdsLv\">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference\n</a> video, as that was a good balance between performance and time of training.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by defining the <code>get_all_sentences</code> function to iterate over the dataset and extract the sentences according to the language pair defined—we will do that later.</p>","metadata":{"id":"YdVFowgUqr_Q"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `get_all_sentences` function to extract sentences from a dataset\n# - Accept parameters:\n#   1. `ds`: The dataset containing translation pairs\n#   2. `lang`: The language key to extract translations\n\n# - Iterate through the dataset:\n#   1. Access the 'translation' field of each pair\n#   2. Yield the sentence corresponding to the specified language key\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef get_all_sentences(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]","metadata":{"id":"xvRuuTpIveZS","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:21:56.625561Z","iopub.execute_input":"2025-01-09T22:21:56.625887Z","iopub.status.idle":"2025-01-09T22:21:56.629894Z","shell.execute_reply.started":"2025-01-09T22:21:56.625856Z","shell.execute_reply":"2025-01-09T22:21:56.628979Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_ds</code> function is defined to load and prepare the dataset for training and validation. In this function, we build or load the tokenizer, split the dataset, and create DataLoaders, so the model can successfully iterate over the dataset in batches. The result of these functions is tokenizers for the source and target languages plus the DataLoader objects.</p>","metadata":{"id":"EA13IRYEqr_R"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `get_ds` function to process and prepare the dataset for training\n# - Load the `OpusBooks` dataset using:\n#   1. Source and target languages from `config`\n#   2. Train split of the dataset\n\n# - Build or load tokenizers for source and target languages using `build_tokenizer`\n\n# - Split the dataset into training and validation sets:\n#   1. Allocate 90% for training and 10% for validation\n#   2. Use `random_split` for randomized splitting\n\n# - Process the splits using a `BilingualDataset` class:\n#   1. Convert sentences to tokenized representations\n#   2. Apply source and target tokenizers\n#   3. Ensure sequence lengths conform to `config`\n\n# - Compute and print the maximum sentence lengths for both source and target languages\n\n# - Create DataLoader objects for training and validation:\n#   1. Define batch sizes from `config`\n#   2. Enable shuffling for training DataLoader\n\n# - Return:\n#   1. Training DataLoader\n#   2. Validation DataLoader\n#   3. Tokenizer for source language\n#   4. Tokenizer for target language\n\n######################  TODO  ########################\n######################  TODO  ########################\n\ndef get_ds(config):\n    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n\n    # Build tokenizers\n    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    # Split dataset into training and validation\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    # Find the maximum length of each sentence in the source and target sentence\n    max_len_src = 0\n    max_len_tgt = 0\n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt","metadata":{"id":"IkTRqP8LvpVy","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:22:06.956108Z","iopub.execute_input":"2025-01-09T22:22:06.956457Z","iopub.status.idle":"2025-01-09T22:22:06.963284Z","shell.execute_reply.started":"2025-01-09T22:22:06.956431Z","shell.execute_reply":"2025-01-09T22:22:06.962252Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We define the <code>casual_mask</code> function to create a mask for the attention mechanism of the decoder. This mask prevents the model from having information about future elements in the sequence. </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by making a square grid filled with ones. We determine the grid size with the <code>size</code> parameter. Then, we change all the numbers above the main diagonal line to zeros. Every number on one side becomes a zero, while the rest remain ones. The function then flips all these values, turning ones into zeros and zeros into ones. This process is crucial for models that predict future tokens in a sequence.</p>","metadata":{"id":"VK3d2-AVqr_R"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `casual_mask` function to create an upper triangular mask\n# - Accept `size` as the dimension of the square matrix\n# - Steps:\n#   1. Create a square matrix of size `size x size` filled with ones\n#   2. Use `torch.triu` to make it upper triangular, with zeros below the diagonal\n#   3. Convert the matrix to integer type\n#   4. Return the mask where zeros represent the causal positions\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef causal_mask(size):\n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n    return mask == 0","metadata":{"id":"kTgMYaY2vvWq","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:22:14.501781Z","iopub.execute_input":"2025-01-09T22:22:14.502061Z","iopub.status.idle":"2025-01-09T22:22:14.506514Z","shell.execute_reply.started":"2025-01-09T22:22:14.502039Z","shell.execute_reply":"2025-01-09T22:22:14.505434Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>BilingualDataset</code> class processes the texts of the target and source languages in the dataset by tokenizing them and adding all the necessary special tokens. This class also certifies that the sentences are within a maximum sequence length for both languages and pads all necessary sentences.</p>","metadata":{"id":"ccdK5XnMqr_R"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `BilingualDataset` class inheriting from `Dataset`\n# - Initialize with:\n#   1. `ds`: Dataset containing sentence pairs\n#   2. `tokenizer_src` and `tokenizer_tgt`: Tokenizers for source and target languages\n#   3. `src_lang` and `tgt_lang`: Language identifiers\n#   4. `seq_len`: Maximum sequence length for tokens\n\n# - Define special tokens (`[SOS]`, `[EOS]`, `[PAD]`) using the target tokenizer\n\n# - Implement `__len__` to return the number of sentence pairs in the dataset\n\n# - Implement `__getitem__` to:\n#   1. Retrieve source and target texts based on the index\n#   2. Tokenize source and target texts\n#   3. Compute required padding for source and target tokens\n#   4. Raise an error if tokenized sentences exceed `seq_len`\n#   5. Build `encoder_input` by concatenating `[SOS]`, tokenized text, `[EOS]`, and padding\n#   6. Build `decoder_input` by concatenating `[SOS]`, tokenized text, and padding\n#   7. Build `label` by concatenating tokenized text, `[EOS]`, and padding\n#   8. Ensure all tensors are of length `seq_len`\n\n# - Return a dictionary containing:\n#   1. `encoder_input`: Tensor for the encoder\n#   2. `decoder_input`: Tensor for the decoder\n#   3. `encoder_mask`: Mask for non-padding tokens in the encoder\n#   4. `decoder_mask`: Mask for non-padding tokens in the decoder with causal masking\n#   5. `label`: Expected output for training\n#   6. `src_text` and `tgt_text`: Original source and target texts\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.seq_len = seq_len\n\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        src_target_pair = self.ds[idx]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n\n        # Transform the text into tokens\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        # Add sos, eos and padding to each sentence\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1  # We will add <s>\n\n        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError(\"Sentence is too long\")\n\n        # Add <s> and </s> token\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n\n        # Add only <s> token\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n\n        # Add only </s> token\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n\n        # Double check the size of the tensors to make sure they are all seq_len long\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            \"encoder_input\": encoder_input,  # (seq_len)\n            \"decoder_input\": decoder_input,  # (seq_len)\n            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),  # (1, seq_len) & (1, seq_len, seq_len)\n            \"label\": label,  # (seq_len)\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text,\n        }","metadata":{"id":"x9v94mdgv3y6","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:22:26.983320Z","iopub.execute_input":"2025-01-09T22:22:26.983624Z","iopub.status.idle":"2025-01-09T22:22:26.993248Z","shell.execute_reply.started":"2025-01-09T22:22:26.983601Z","shell.execute_reply":"2025-01-09T22:22:26.992227Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Part 12: Validation Loop","metadata":{"id":"B7cXlNUfv5uL"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now create two functions for the validation loop. The validation loop is crucial to evaluate model performance in translating sentences from data it has not seen during training.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will define two functions. The first function, <code>greedy_decode</code>, gives us the model's output by obtaining the most probable next token. The second function, <code>run_validation</code>, is responsible for running the validation process in which we decode the model's output and compare it with the reference text for the target sentence.</p>","metadata":{"id":"tf8Wt860qr_R"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `greedy_decode` function to generate the most probable sequence using a trained model\n# - Accept parameters:\n#   1. `model`: Trained Transformer model\n#   2. `source`: Source input sequence\n#   3. `source_mask`: Mask for the source sequence\n#   4. `tokenizer_src` and `tokenizer_tgt`: Tokenizers for source and target languages\n#   5. `max_len`: Maximum sequence length for the output\n#   6. `device`: Device to run the computation (e.g., CPU or GPU)\n\n# - Steps:\n#   1. Retrieve indices for `[SOS]` and `[EOS]` tokens from the target tokenizer\n#   2. Compute encoder output for the source sequence\n#   3. Initialize decoder input with `[SOS]`\n#   4. Loop until `max_len` is reached or `[EOS]` is generated:\n#      - Create a causal mask for the decoder input\n#      - Compute decoder output using encoder output and masks\n#      - Apply the projection layer to get probabilities for the next token\n#      - Select the token with the highest probability and append it to the decoder input\n#      - Break the loop if `[EOS]` is generated\n#   5. Return the generated sequence of tokens\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    # Precompute the encoder output and reuse it for every step\n    encoder_output = model.encode(source, source_mask)\n    # Initialize the decoder input with the sos token\n    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n\n        # Build mask for target\n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        # Calculate output\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        # Get next token\n        prob = model.project(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)","metadata":{"id":"z1rzcAkpv8Ew","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:22:34.705473Z","iopub.execute_input":"2025-01-09T22:22:34.705752Z","iopub.status.idle":"2025-01-09T22:22:34.711856Z","shell.execute_reply.started":"2025-01-09T22:22:34.705731Z","shell.execute_reply":"2025-01-09T22:22:34.710868Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `run_validation` function to evaluate the model on the validation dataset\n# - Accept parameters:\n#   1. `model`: Trained Transformer model\n#   2. `validation_ds`: Validation dataset\n#   3. `tokenizer_src` and `tokenizer_tgt`: Tokenizers for source and target languages\n#   4. `max_len`: Maximum sequence length for decoding\n#   5. `device`: Device to run the computation (e.g., CPU or GPU)\n#   6. `print_msg`: Function for displaying output messages\n#   7. `global_state`: Optional global state for tracking progress\n#   8. `writer`: Optional logging writer (e.g., TensorBoard)\n#   9. `num_examples`: Number of examples to process per run (default: 2)\n\n# - Steps:\n#   1. Set the model to evaluation mode\n#   2. Initialize a counter to track the number of processed examples\n#   3. Define a fixed console width for printed messages\n#   4. Iterate through the validation dataset:\n#      - Retrieve `encoder_input` and `encoder_mask` and move them to the specified device\n#      - Ensure batch size is 1 for validation\n#      - Use `greedy_decode` to generate the model's predictions\n#      - Decode the model's output into human-readable text\n#      - Print source, target, and predicted text using `print_msg`\n#      - Break the loop after processing the specified number of examples (`num_examples`)\n#   5. Ensure no gradients are computed during evaluation\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n    model.eval()\n    count = 0\n\n    console_width = 80\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)  # (batch_size, seq_len)\n            encoder_mask = batch[\"encoder_mask\"].to(device)  # (batch_size, 1, 1, seq_len)\n\n            # Check that the batch size is 1\n            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n\n            # Print the source, target and model output\n            print_msg('-' * console_width)\n            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n\n            if count == num_examples:\n                break","metadata":{"id":"iF7v9L0owcLT","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:22:43.600868Z","iopub.execute_input":"2025-01-09T22:22:43.601201Z","iopub.status.idle":"2025-01-09T22:22:43.607673Z","shell.execute_reply.started":"2025-01-09T22:22:43.601138Z","shell.execute_reply":"2025-01-09T22:22:43.606690Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Part 13: Training Loop","metadata":{"id":"qw3nykKxwkIh"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We are ready to train our Transformer model on the OpusBook dataset for the English to Italian translation task.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We first start by defining the <code>get_model</code> function to load the model by calling the <code>build_transformer</code> function we have previously defined. This function uses the <code>config</code> dictionary to set a few parameters.</p>","metadata":{"id":"az_Kwq4Zqr_S"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `get_model` function to initialize a Transformer model\n# - Accept parameters:\n#   1. `config`: Configuration dictionary with model settings\n#   2. `vocab_src_len`: Length of the source language vocabulary\n#   3. `vocab_tgt_len`: Length of the target language vocabulary\n\n# - Use the `build_transformer` function to:\n#   1. Create a Transformer model\n#   2. Pass the source and target vocabulary lengths\n#   3. Set sequence length (`seq_len`) and embedding dimensionality (`d_model`) from `config`\n\n# - Return the initialized model\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model","metadata":{"id":"7QMn1BULwnBl","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:22:56.309341Z","iopub.execute_input":"2025-01-09T22:22:56.309623Z","iopub.status.idle":"2025-01-09T22:22:56.313900Z","shell.execute_reply.started":"2025-01-09T22:22:56.309600Z","shell.execute_reply":"2025-01-09T22:22:56.313113Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I have mentioned the <code>config</code> dictionary several times throughout this notebook. Now, it is time to create it.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the following cell, we will define two functions to configure our model and the training process.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>get_config</code> function, we define crucial parameters for the training process. <code>batch_size</code> for the number of training examples used in one iteration, <code>num_epochs</code> as the number of times the entire dataset is passed forward and backward through the Transformer, <code>lr</code> as the learning rate for the optimizer, etc. We will also finally define the pairs from the OpusBook dataset, <code>'lang_src': 'en'</code> for selecting English as the source language and <code>'lang_tgt': 'it'</code> for selecting Italian as the target language.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_weights_file_path</code> function constructs the file path for saving or loading model weights for any specific epoch.</p>","metadata":{"id":"Ord2DlVkqr_S"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `get_config` function to return a dictionary of settings for building and training the Transformer model:\n#   1. `batch_size`: Number of samples per training batch\n#   2. `num_epochs`: Total training epochs\n#   3. `lr`: Learning rate for optimization\n#   4. `seq_len`: Maximum sequence length for tokens\n#   5. `d_model`: Dimensionality of embeddings (e.g., 512)\n#   6. `lang_src` and `lang_tgt`: Source and target languages\n#   7. `model_folder`: Folder to save model weights\n#   8. `model_basename`: Base name for model files\n#   9. `preload`: Option to preload a model (default: None)\n#   10. `tokenizer_file`: Filename pattern for saving tokenizers\n#   11. `experiment_name`: Name of the experiment for logging\n\n# - Define `get_weights_file_path` to construct a file path for saving/retrieving model weights:\n#   1. Accept `config` dictionary and `epoch` string as parameters\n#   2. Retrieve `model_folder` and `model_basename` from `config`\n#   3. Construct the filename with the base name and epoch\n#   4. Combine the current directory, model folder, and filename to return the full path\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef get_config():\n    return {\n        \"batch_size\": 24,\n        \"num_epochs\": 4,\n        \"lr\": 10**-4,\n        \"seq_len\": 350,\n        \"d_model\": 512,\n        \"lang_src\": \"en\",\n        \"lang_tgt\": \"it\",\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\",\n        \"preload\": None,\n        \"tokenizer_file\": \"tokenizer_{0}.json\",\n        \"experiment_name\": \"runs/tmodel\"\n    }\n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder']\n    model_basename = config['model_basename']\n    model_filename = f\"{model_basename}{epoch}.pt\"\n    return str(Path('.') / model_folder / model_filename)","metadata":{"id":"gXt82CejxeHZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:23:38.360596Z","iopub.execute_input":"2025-01-09T22:23:38.360881Z","iopub.status.idle":"2025-01-09T22:23:38.366195Z","shell.execute_reply.started":"2025-01-09T22:23:38.360860Z","shell.execute_reply":"2025-01-09T22:23:38.365229Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally define our last function, <code>train_model</code>, which takes the <code>config</code> arguments as input. </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In this function, we will set everything up for the training. We will load the model and its necessary components onto the GPU for faster training, set the <code>Adam</code> optimizer, and configure the <code>CrossEntropyLoss</code> function to compute the differences between the translations output by the model and the reference translations from the dataset. </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Every loop necessary for iterating over the training batches, performing backpropagation, and computing the gradients is in this function. We will also use it to run the validation function and save the current state of the model.</p>","metadata":{"id":"Qw7SjmrDqr_S"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Define a `train_model` function to train a Transformer model\n# - Steps:\n#   1. Set up the device (GPU or CPU) for training\n#   2. Create a directory to store model weights\n#   3. Retrieve dataloaders and tokenizers for source and target languages using `get_ds`\n#   4. Initialize the Transformer model using `get_model` and move it to the specified device\n#   5. Set up TensorBoard for logging training metrics\n#   6. Configure the Adam optimizer with learning rate and epsilon from `config`\n#   7. If a pre-trained model exists:\n#      - Load the model, optimizer state, and global step\n#      - Set the starting epoch for resuming training\n#   8. Define a cross-entropy loss function:\n#      - Ignore padding tokens\n#      - Apply label smoothing to prevent overfitting\n#   9. Start training loop:\n#      - Iterate over epochs from the initial epoch to `config['num_epochs']`\n#      - For each batch in the training dataloader:\n#         - Set model to training mode\n#         - Move input data, masks, and labels to the device\n#         - Pass data through the encoder, decoder, and projection layer\n#         - Compute loss between model predictions and labels\n#         - Log training loss to TensorBoard\n#         - Perform backpropagation and update model parameters\n#         - Clear gradients for the next batch\n#         - Increment global step counter\n#      - After each epoch, run validation using `run_validation`\n#      - Save the current model state, optimizer state, and global step\n#   10. Save model weights after each epoch\n\n######################  TODO  ########################\n######################  TODO  ########################\ndef train_model(config):\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Create the model folder if it doesn't exist\n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n\n    # Load the dataset\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n\n    # Initialize the model\n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n\n    # Set up TensorBoard\n    writer = SummaryWriter(config['experiment_name'])\n\n    # Define the optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n\n    # If a pre-trained model exists, load it\n    initial_epoch = 0\n    global_step = 0\n    if config['preload'] is not None:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename)\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n\n    # Define the loss function\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_tgt.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n    # Training loop\n    for epoch in range(initial_epoch, config['num_epochs']):\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        for batch in batch_iterator:\n            encoder_input = batch['encoder_input'].to(device)  # (batch_size, seq_len)\n            decoder_input = batch['decoder_input'].to(device)  # (batch_size, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device)  # (batch_size, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device)  # (batch_size, 1, seq_len, seq_len)\n\n            # Run the tensors through the encoder, decoder, and projection layer\n            encoder_output = model.encode(encoder_input, encoder_mask)  # (batch_size, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)  # (batch_size, seq_len, d_model)\n            proj_output = model.project(decoder_output)  # (batch_size, seq_len, vocab_size)\n\n            # Get the label\n            label = batch['label'].to(device)  # (batch_size, seq_len)\n\n            # Compute the loss\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n            # Log the loss\n            writer.add_scalar('train_loss', loss.item(), global_step)\n            writer.flush()\n\n            # Backpropagate the loss\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n            optimizer.zero_grad()\n\n            global_step += 1\n\n        # Run validation at the end of every epoch\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n        # Save the model at the end of every epoch\n        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)","metadata":{"id":"2qK9wAjRxoDQ","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:23:51.707033Z","iopub.execute_input":"2025-01-09T22:23:51.707391Z","iopub.status.idle":"2025-01-09T22:23:51.717660Z","shell.execute_reply.started":"2025-01-09T22:23:51.707363Z","shell.execute_reply":"2025-01-09T22:23:51.716730Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"We can now train the model!","metadata":{"id":"nrMmfyi8xrXw"}},{"cell_type":"code","source":"if __name__ == '__main__':\n    warnings.filterwarnings('ignore')\n    config = get_config()\n    train_model(config)\n    # pass","metadata":{"id":"28425EYaxrsi","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"aad25666-a1d3-4ec0-c7d0-865ba9b55c81","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T22:23:58.981183Z","iopub.execute_input":"2025-01-09T22:23:58.981511Z","iopub.status.idle":"2025-01-09T23:20:24.905528Z","shell.execute_reply.started":"2025-01-09T22:23:58.981485Z","shell.execute_reply":"2025-01-09T23:20:24.904694Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"658e569156c04f149877969a7366190b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47315bd02dd24c1d8b4fe687aa79fc05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ddd35046085465ab6944ddcda44f4a3"}},"metadata":{}},{"name":"stdout","text":"Max length of source sentence: 309\nMax length of target sentence: 274\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 00: 100%|██████████| 1213/1213 [13:59<00:00,  1.44it/s, loss=6.000]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: Mr. Brocklehurst resumed. \"This I learned from her benefactress; from the pious and charitable lady who adopted her in her orphan state, reared her as her own daughter, and whose kindness, whose generosity the unhappy girl repaid by an ingratitude so bad, so dreadful, that at last her excellent patroness was obliged to separate her from her own young ones, fearful lest her vicious example should contaminate their purity: she has sent her here to be healed, even as the Jews of old sent their diseased to the troubled pool of Bethesda; and, teachers, superintendent, I beg of you not to allow the waters to stagnate round her.\"\n    TARGET: — Tutte queste cose, — aggiunse il signor Bockelhurst, — le ho sapute dalla sua benefattrice, quella pia e caritatevole signora, che l'ha adottata quando rimase orfana, e l'ha educata insieme con le sue figlie; e questa disgraziata bambina ha pagata la sua bontà e la sua generosità con una ingratitudine così grande, che l'eccellente signora Reed è stata costretta di separare Jane dai suoi figli, affinchè il suo esempio non contaminasse la loro purezza. È stata mandata qui per essere guarita, come gli ebrei mandano i loro malati al lago di Betteda.\n PREDICTED: — Non è un ’ ic , ma non si , e il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo , e , e , e , e , e , e , e che si , e che si , e , e , e il suo suo suo suo suo suo suo , e non era , e , e , e il suo suo suo suo suo tratto , e , e , e , e , e la sua , e , e il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo ’ era , e , e non era , e la suo , e , e che si , e , e la , e non era , e , e , e non era , e che si , e non era la , e non era , e non era , e non era , e che si , e non era\n--------------------------------------------------------------------------------\n    SOURCE: \"Go to the devil!\" was his brother-in-law's recommendation.\n    TARGET: — Andate al diavolo, — gli disse il cognato.\n PREDICTED: — Non è un ’ ic , — disse il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo sorriso .\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 01: 100%|██████████| 1213/1213 [14:00<00:00,  1.44it/s, loss=5.372]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: \"I am a fool!\" cried Mr. Rochester suddenly. \"I keep telling her I am not married, and do not explain to her why.\n    TARGET: — Sono un pazzo! — esclamò a un tratto il signor Rochester. — Le dico che non sono ammogliato, e non le spiego il perché.\n PREDICTED: — Io , — disse il signor Rochester , — ma non ho detto che non ho detto che non ho detto che non posso essere mai .\n--------------------------------------------------------------------------------\n    SOURCE: It had lost half its tail, one of its ears, and a fairly appreciable proportion of its nose.\n    TARGET: Aveva perduto la coda, un orecchio, e una parte del naso.\n PREDICTED: Era un ’ altra , e , e il suo , e .\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 1213/1213 [14:00<00:00,  1.44it/s, loss=5.006]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: I wondered why moralists call this world a dreary wilderness: for me it blossomed like a rose.\n    TARGET: \"Chiedevo a me stessa perché i filosofi chiamino il mondo un triste deserto; a me pareva pieno di fiori.\n PREDICTED: Io non mi a , e mi , mi .\n--------------------------------------------------------------------------------\n    SOURCE: Mr. Brocklehurst resumed. \"This I learned from her benefactress; from the pious and charitable lady who adopted her in her orphan state, reared her as her own daughter, and whose kindness, whose generosity the unhappy girl repaid by an ingratitude so bad, so dreadful, that at last her excellent patroness was obliged to separate her from her own young ones, fearful lest her vicious example should contaminate their purity: she has sent her here to be healed, even as the Jews of old sent their diseased to the troubled pool of Bethesda; and, teachers, superintendent, I beg of you not to allow the waters to stagnate round her.\"\n    TARGET: — Tutte queste cose, — aggiunse il signor Bockelhurst, — le ho sapute dalla sua benefattrice, quella pia e caritatevole signora, che l'ha adottata quando rimase orfana, e l'ha educata insieme con le sue figlie; e questa disgraziata bambina ha pagata la sua bontà e la sua generosità con una ingratitudine così grande, che l'eccellente signora Reed è stata costretta di separare Jane dai suoi figli, affinchè il suo esempio non contaminasse la loro purezza. È stata mandata qui per essere guarita, come gli ebrei mandano i loro malati al lago di Betteda.\n PREDICTED: \" La signora Fairfax mi ha detto che la signora Fairfax , e la signora Fairfax si , e la sua madre , che la sua madre , e la sua madre , che non è più di un tratto , e la sua vita , e la sua vita , che la sua vita è la sua vita , e la sua vita , e la sua vita è stato di , e la sua vita , e la sua vita , e la sua vita , e la sua vita , non è più , e la sua vita , e la sua vita , e la sua , e la sua vita , e la sua vita è la sua vita .\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03: 100%|██████████| 1213/1213 [14:00<00:00,  1.44it/s, loss=5.122]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: After making sure he had missed, he turned and saw that the trap and horses were no longer on the road but in the marsh.\n    TARGET: Convinto d’aver fatto padella, Levin si voltò e vide che i cavalli col calesse non erano più sulla strada, ma nella palude.\n PREDICTED: Dopo aver visto il tempo , si era già già già già già già già già già già già più di lui , ma non era stato .\n--------------------------------------------------------------------------------\n    SOURCE: It's a banjo.\"\n    TARGET: È un banjo.\n PREDICTED: È un po ’ di tè .\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"%ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T23:22:01.733924Z","iopub.execute_input":"2025-01-09T23:22:01.734884Z","iopub.status.idle":"2025-01-09T23:22:01.899305Z","shell.execute_reply.started":"2025-01-09T23:22:01.734835Z","shell.execute_reply":"2025-01-09T23:22:01.898091Z"}},"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mruns\u001b[0m/  tokenizer_en.json  tokenizer_it.json  \u001b[01;34mweights\u001b[0m/\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Section 2: BERT and LoRA\n\nWelcome to Section 2 of our Machine Learning assignment! I hope you've been enjoying the journey so far! 😊\n\n In this section, you will gain hands-on experience with [BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) and [LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaptation) for text classification tasks. The section is divided into three main parts, each focusing on different aspects of NLP techniques.\n\n## Assignment Structure\n\n### Part 1: Data Preparation and Preprocessing\nIn this part, you will work with a text classification dataset. You will learn how to:\n- Download and load the dataset\n- Perform necessary preprocessing steps\n- Implement data cleaning and transformation techniques\n- Prepare the data in a format suitable for BERT training\n\n### Part 2: Building a Small BERT Model\nYou will create and train a small BERT model from scratch using the Hugging Face [Transformers](https://huggingface.co/docs/transformers/en/index) library. This part will help you understand:\n- The architecture of BERT\n- How to configure and initialize a BERT model\n- Training process and optimization\n- Model evaluation and performance analysis\n\n### Part 3: Fine-tuning with LoRA\nIn the final part, you will work with a pre-trained [TinyBERT](https://arxiv.org/abs/1909.10351) model and use LoRA for efficient fine-tuning. You will:\n- Load a pre-trained TinyBERT model\n- Implement LoRA adaptation and fine-tune the model on our classification task\n- Compare the results with the previous approach","metadata":{"id":"v3axMN7QWiVH"}},{"cell_type":"markdown","source":"---\n\n> **NOTE**:  \n> Throughout this notebook, make an effort to include sufficient visualizations to enhance understanding:  \n> - In the data processing section, display the results of your operations (e.g., show data samples or distributions after preprocessing).  \n> - In the classification section, report various evaluation metrics such as accuracy, precision, recall, and F1-score to thoroughly assess your model's performance.  \n> - Additionally, take a moment to compare the sizes of the models discussed in this notebook with today’s enormous models. This will help you appreciate the challenges and computational demands associated with training such massive models. 😵‍💫\n\n---\n","metadata":{"id":"M6FKcSFbOTMd"}},{"cell_type":"markdown","source":"## Part 1: Data Preparation and Preprocessing\nWe'll be working with the [Consumer Complaint](https://catalog.data.gov/dataset/consumer-complaint-database) dataset, which contains ***complaints*** submitted by consumers about financial products and services. Our goal is to build a classifier that can automatically identify the type of complaint based on the consumer's text description. For this task, we will work with a smaller subset of the dataset, available for download through this [link](https://drive.google.com/file/d/1SpIHksR-WzruEgUjp1SQKGG8bZPnJJoN/view?usp=sharing).","metadata":{"id":"GHKw2r6yYV7n"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gdown\nimport string\nimport re\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom peft import get_peft_model, LoraConfig, TaskType","metadata":{"id":"7ELMR8kXUh3o"},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 1.2 Loading the Data","metadata":{"id":"9oJXlKLYeymq"}},{"cell_type":"code","source":"file_id = \"1SpIHksR-WzruEgUjp1SQKGG8bZPnJJoN\"  # Replace this with your file's ID\noutput_file = \"complaints_small.zip\"  # Replace \"data_file.ext\" with the desired output filename and extension\n\ngdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"sTXraf1Iu1TR","outputId":"eb4e814d-a384-46da-cdf4-3658a98cffae"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1SpIHksR-WzruEgUjp1SQKGG8bZPnJJoN\n","From (redirected): https://drive.google.com/uc?id=1SpIHksR-WzruEgUjp1SQKGG8bZPnJJoN&confirm=t&uuid=8bda4fb9-bc74-4c6a-b499-d1b4e2a62d31\n","To: /content/complaints_small.zip\n","100%|██████████| 290M/290M [00:13<00:00, 20.8MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'complaints_small.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"execution_count":2},{"cell_type":"code","source":"!unzip complaints_small.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEO8p_6tvArJ","outputId":"197eb229-54a0-4726-c562-3aba171d99ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  complaints_small.zip\n","  inflating: complaints_small.csv    \n"]}],"execution_count":3},{"cell_type":"code","source":"%ls","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5yzLdE1vG7H","outputId":"e152c5e5-dfd6-424f-e00a-012d41932235"},"outputs":[{"output_type":"stream","name":"stdout","text":["complaints_small.csv  complaints_small.zip  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"]}],"execution_count":4},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n# Load the dataset\n\n######################  TODO  ########################\n######################  TODO  ########################\ndf = pd.read_csv('complaints_small.csv')\ndf.head()","metadata":{"id":"mGga8BmnUcl0","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"a26c3aba-f858-4208-d87f-1d01c4e52158"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Product  \\\n","0  Credit reporting, credit repair services, or o...   \n","1                                       Student loan   \n","2  Credit reporting or other personal consumer re...   \n","3  Credit reporting, credit repair services, or o...   \n","4  Credit reporting or other personal consumer re...   \n","\n","                        Consumer complaint narrative  \n","0  My credit reports are inaccurate. These inaccu...  \n","1  Beginning in XX/XX/XXXX I had taken out studen...  \n","2  I am disputing a charge-off on my account that...  \n","3  I did not consent to, authorize, nor benefit f...  \n","4  I am a federally protected consumer and I am a...  "],"text/html":["\n","  <div id=\"df-a06b26dc-c302-4542-88cd-e91565498fc5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Product</th>\n","      <th>Consumer complaint narrative</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Credit reporting, credit repair services, or o...</td>\n","      <td>My credit reports are inaccurate. These inaccu...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Student loan</td>\n","      <td>Beginning in XX/XX/XXXX I had taken out studen...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Credit reporting or other personal consumer re...</td>\n","      <td>I am disputing a charge-off on my account that...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Credit reporting, credit repair services, or o...</td>\n","      <td>I did not consent to, authorize, nor benefit f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Credit reporting or other personal consumer re...</td>\n","      <td>I am a federally protected consumer and I am a...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a06b26dc-c302-4542-88cd-e91565498fc5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a06b26dc-c302-4542-88cd-e91565498fc5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a06b26dc-c302-4542-88cd-e91565498fc5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a0ffd2ad-cc7d-42b2-a39f-cd5e177f014c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0ffd2ad-cc7d-42b2-a39f-cd5e177f014c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a0ffd2ad-cc7d-42b2-a39f-cd5e177f014c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":5}],"execution_count":5},{"cell_type":"markdown","source":"### 1.3 Data Sampling and Class Distribution Analysis\n\nWorking with large datasets can be computationally intensive during development. Additionally, imbalanced class distribution can affect model performance. In this section, you'll sample the data and analyze class distributions to make informed decisions about your training dataset.","metadata":{"id":"L9hr8-FNgpVO"}},{"cell_type":"markdown","source":"---\n\nWe'll work with a manageable portion of the data to develop and test our approach. While using the complete dataset would likely yield better results, a smaller sample allows us to prototype our solution more efficiently.\n","metadata":{"id":"Cl_g_ZU4h5RG"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Sample a portion of the complete dataset\n# - Display the first few rows of your sampled dataset\n# - Print the shape of your original and sampled datasets\n\n######################  TODO  ########################\n######################  TODO  ########################\nprint(\"Original dataset shape:\", df.shape)\ndf_sample = df.sample(n=10000, random_state=42)\nprint(\"Sampled dataset shape:\", df_sample.shape)\ndf_sample.head(5)","metadata":{"id":"QAJUXNCFhYsf","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"36cdab47-f830-4197-e522-2f3c222ed254"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original dataset shape: (941128, 2)\n","Sampled dataset shape: (10000, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                  Product  \\\n","335123  Credit reporting or other personal consumer re...   \n","601718                                           Mortgage   \n","847752  Credit reporting, credit repair services, or o...   \n","765316  Credit reporting or other personal consumer re...   \n","798300  Credit reporting, credit repair services, or o...   \n","\n","                             Consumer complaint narrative  \n","335123  Upon reviewing my credit report, I have identi...  \n","601718  I was doing a rate check to refinance. The age...  \n","847752  This is my 2nd request that I have been a vict...  \n","765316  I'm sending this compliant to inform credit bu...  \n","798300  Im submitting a complaint to you today to info...  "],"text/html":["\n","  <div id=\"df-5dc3c693-adae-4f29-92c7-d0141bb74f5e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Product</th>\n","      <th>Consumer complaint narrative</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>335123</th>\n","      <td>Credit reporting or other personal consumer re...</td>\n","      <td>Upon reviewing my credit report, I have identi...</td>\n","    </tr>\n","    <tr>\n","      <th>601718</th>\n","      <td>Mortgage</td>\n","      <td>I was doing a rate check to refinance. The age...</td>\n","    </tr>\n","    <tr>\n","      <th>847752</th>\n","      <td>Credit reporting, credit repair services, or o...</td>\n","      <td>This is my 2nd request that I have been a vict...</td>\n","    </tr>\n","    <tr>\n","      <th>765316</th>\n","      <td>Credit reporting or other personal consumer re...</td>\n","      <td>I'm sending this compliant to inform credit bu...</td>\n","    </tr>\n","    <tr>\n","      <th>798300</th>\n","      <td>Credit reporting, credit repair services, or o...</td>\n","      <td>Im submitting a complaint to you today to info...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dc3c693-adae-4f29-92c7-d0141bb74f5e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5dc3c693-adae-4f29-92c7-d0141bb74f5e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5dc3c693-adae-4f29-92c7-d0141bb74f5e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ace89648-b58d-4487-a9f5-e19c8a3de7cb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ace89648-b58d-4487-a9f5-e19c8a3de7cb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ace89648-b58d-4487-a9f5-e19c8a3de7cb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_sample","summary":"{\n  \"name\": \"df_sample\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"Credit reporting or other personal consumer reports\",\n          \"Checking or savings account\",\n          \"Consumer Loan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Consumer complaint narrative\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9231,\n        \"samples\": [\n          \"USAA randomly chooses to hold credit card payments for up to ten days. The is the second time I have had my payment held. This time, my high utilization affected my credit score even though I made the payment.\\n\\nI have called and asked three different representatives who have given me three different answers. One told me I have to sign up for a bank account to get rid of the issue. One told me that they as a company were not able to control the algorithms that withhold payments. And the last one told me that the federal reserve was the reason why they would not credit my account with my payment.\\n\\nIt is part of the terms of agreement that they can hold payments for up to ten days. But now they are implementing this more frequently. That, and they will not give me a reason as to why. I have had the credit account for more than 6 years and used the same bank account to make payments on this card. It is only recently that they have started implementing this hold.\\n\\nI do find it necessary to file this complaint because it is an abusive policy written in the fine print. If I knew they would do this I never would have begun doing business with USAA. I have no idea whether USAA will withhold my assets from me and for how long when I use my card, it causes all sorts of problems with automatic bill pay. For those who are less financially stable, putting a ten day hold on a payment could be very difficult.\\n\\nThank you\",\n          \"I am a victim of identity theft and this debt does not belong to me. \\n\\nPlease see the identity theft report and legal affidavit attached.\",\n          \"I have received a collection notice for {$110.00} from Tate & Kirlin Associates Inc. of XXXX, PA. The Current Creditor Listed is XXXX XXXX. Reference # XXXX. The notice is dated XXXX XXXX XXXX, and was received on XX/XX/XXXX. \\n\\nI have received calls alleging tht I owe for a \\\" XXXX XXXX, '' which I had never requested, nor received, nor has anyone else in my household. I have no record of any contract with this company, nor have they provided one. I also had never received an invoice, \\\" past due '' notice or even an email. The collection notice was the first and only mail I have received.\\n\\nThe closest thing that I have received is a Fraud Alert from XXXX XXXX  on XX/XX/XXXX. \\nBelow is a copy of that alert, name and card number redacted : -- -- -- -- -- -- - Action Needed : Please confirm activity XXXX  Fraud Protection Services : XXXX debit or ATM card ending in [ # # # # ] [ name ] Did you or someone you authorized use your XXXX  debit or ATM card for : A transaction of {$110.00} that was declined at a subscription merchant INTRAPOWERBREAKTHROUGH in XXXX, CA XXXX on XX/XX/XXXX -- -- -- -- -- -- -- Aditionally, cursory Internet research indicates less-than-exemplary reputations, with a number of claims of unethical, possibly illegal activities against them. I am not qualified to make such a judgment, but it seems prudent to proceed with caution! \\n\\nAll this leads me to believe that an attempt to perpetrate fraud is being attempted. Your attention to this is greatly appreciated!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}],"execution_count":6},{"cell_type":"markdown","source":"---\n\nLet's examine the distribution of ***complaints*** types in our dataset. You'll notice that some products have significantly more instances than others, and some categories are quite similar. For example:\n\n- Multiple categories might refer to similar financial products\n- Some categories might have very few examples\n- Certain categories might be subcategories of others\n\nYou have two main approaches to handle this situation:\n\n1. **Merging Similar Classes:** Identify categories that represent similar products/services and Combine them to create more robust, general categories\n\n2. **Selecting Major Classes:** Only select the categories with sufficient representation\n\n\n\n> You may choose any approach, but after this step, your data must include **at least five** distinct classes.\n\n","metadata":{"id":"50a4NJeMiBb6"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# - Display the number of complaints in each product category\n# - Identify which classes are under-represented\n\n# - Handle class imbalance by choosing and implementing one of these approaches:\n#   1. Merge similar product categories (e.g., combining related categories)\n#   2. Keep only the major classes with sufficient examples\n\n######################  TODO  ########################\n######################  TODO  ########################\n# Analyze class distribution\nproduct_counts = df_sample['Product'].value_counts()\nprint(product_counts)\nprint(\"--------------------------------------------\")\n\n# Handle class imbalance by selecting major classes\ntop_products = product_counts.head(5).index.tolist()\ndf_filtered = df_sample[df_sample['Product'].isin(top_products)]\nprint(df_filtered['Product'].value_counts())","metadata":{"id":"nby2Hrwwjd46","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b00b5cce-91f2-4331-f439-e3e269ed0a76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Product\n","Credit reporting, credit repair services, or other personal consumer reports    3366\n","Credit reporting or other personal consumer reports                             2675\n","Debt collection                                                                 1300\n","Mortgage                                                                         520\n","Checking or savings account                                                      487\n","Credit card or prepaid card                                                      455\n","Credit card                                                                      241\n","Money transfer, virtual currency, or money service                               222\n","Student loan                                                                     192\n","Vehicle loan or lease                                                            164\n","Credit reporting                                                                 123\n","Payday loan, title loan, or personal loan                                         81\n","Bank account or service                                                           71\n","Consumer Loan                                                                     36\n","Payday loan, title loan, personal loan, or advance loan                           28\n","Prepaid card                                                                      20\n","Debt or credit management                                                         10\n","Money transfers                                                                    5\n","Payday loan                                                                        4\n","Name: count, dtype: int64\n","--------------------------------------------\n","Product\n","Credit reporting, credit repair services, or other personal consumer reports    3366\n","Credit reporting or other personal consumer reports                             2675\n","Debt collection                                                                 1300\n","Mortgage                                                                         520\n","Checking or savings account                                                      487\n","Name: count, dtype: int64\n"]}],"execution_count":7},{"cell_type":"markdown","source":"---\n### 1.4 Data Encoding and Text Preprocessing\n\nBefore training our model, we need to prepare both our target labels and text data. This involves converting categorical labels into numerical format and cleaning our text data to improve model performance.","metadata":{"id":"lD3oISsijt1P"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# Label Encoding\n# - Apply label encoding to convert product categories into numeric values\n\n# Text Preprocessing\n# Choose and implement preprocessing steps that you think will improve the quality of your text data.\n# Here are some suggestions:\n\n# - Remove special characters and punctuation\n# - Remove very short complaints (e.g., less than 10 words)\n# - Remove HTML tags if present\n\n######################  TODO  ########################\n######################  TODO  ########################\n# Label Encoding\nlabel_encoder = LabelEncoder()\ndf_filtered['label'] = label_encoder.fit_transform(df_filtered['Product'])\nprint(label_encoder.classes_)\n\n# Text Preprocessing\ndef clean_text(text):\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # remove words with less than 3 letters\n    return text.strip()\n\ndf_filtered['clean_complaint'] = df_filtered['Consumer complaint narrative'].apply(clean_text)\n\n# Remove very short complaints\ndf_filtered = df_filtered[df_filtered['clean_complaint'].apply(lambda x: len(x.split()) >= 10)]\nprint(df_filtered.shape)","metadata":{"id":"pAmaRU92mGyT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48abd186-e2b6-4dcf-ccce-9c500af603ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Checking or savings account'\n"," 'Credit reporting or other personal consumer reports'\n"," 'Credit reporting, credit repair services, or other personal consumer reports'\n"," 'Debt collection' 'Mortgage']\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-4fee44703692>:19: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_filtered['label'] = label_encoder.fit_transform(df_filtered['Product'])\n"]},{"output_type":"stream","name":"stdout","text":["(8184, 4)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-4fee44703692>:29: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_filtered['clean_complaint'] = df_filtered['Consumer complaint narrative'].apply(clean_text)\n"]}],"execution_count":8},{"cell_type":"markdown","source":"## 1.5 Dataset Creation and Tokenization\n\nFor training our BERT model, we need to:\n1. Create a custom Dataset class that will handle tokenization\n2. Split the data into training and testing sets\n3. Use BERT's tokenizer to convert text into a format suitable for the model","metadata":{"id":"j4jVvN4oopUU"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# class ComplaintDataset(Dataset):\n#     \"\"\"A custom Dataset class for handling consumer complaints text data with BERT tokenization.\n\n#     Parameters:\n#         texts (List[str]): List of complaint texts to be processed\n#         labels (List[int]): List of encoded labels corresponding to each text\n#         tokenizer (BertTokenizer): A BERT tokenizer instance for text processing\n#         max_len (int, optional): Maximum length for padding/truncating texts. Defaults to 512\n\n#     Returns:\n#         dict: For each item, returns a dictionary containing:\n#             - input_ids (torch.Tensor): Encoded token ids of the text\n#             - attention_mask (torch.Tensor): Attention mask for the padded sequence\n#             - labels (torch.Tensor): Encoded label as a tensor\n#     \"\"\"\n#     def __init__(self, texts, labels, tokenizer, max_len=512):\n#         ## TODO ##\n#         pass\n\n#     def __len__(self):\n#         ## TODO ##\n#         pass\n\n#     def __getitem__(self, idx):\n#         ## TODO ##\n#         pass\n\n######################  TODO  ########################\n######################  TODO  ########################\nclass ComplaintDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt',\n        )\n\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"id":"yHLQgJhopEh5"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# # Split the data\n# X_train, X_test, y_train, y_test = # TODO\n\n# # Initialize tokenizer and create datasets\n# tokenizer = # TODO\n# train_dataset = # TODO\n# test_dataset = # TODO\n\n# # Create dataloaders\n# train_loader = # TODO\n# test_loader = # TODO\n\n######################  TODO  ########################\n######################  TODO  ########################\n# Split the data\ntexts = df_filtered['clean_complaint'].tolist()\nlabels = df_filtered['label'].tolist()\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Initialize tokenizer and create datasets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_dataset = ComplaintDataset(X_train, y_train, tokenizer)\ntest_dataset = ComplaintDataset(X_test, y_test, tokenizer)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"id":"-js5x8M5mksA","colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["ea97689d8f824f7a947b70751b93dbb1","94e31db2368a4d548954d1793f30e589","2eb227132aa74f55bb6899329529771b","09a0e6ef2d8549598d22717d06b75772","7919d4f46f34495894693ac7e8bd1192","5c54d1926940493b9aeb3cd490da9682","0b4e610b2cc64aa796d2afac1e61866c","ffde291413ab49679d2267a908dab8c9","c8af2085455c487ba0361b227df4dd91","04715d728f004251bbdef5cd2832a245","d9131c294da240d5aead8949add8c8fa","2f1d069a3c2a48ab94981d7a01b27bc3","84102f43c5014323ad908c87c656b099","992a7fcaba774db898bea350ad055eac","520103989b144fafb68872b56d94d939","d350698f76b04fd0ab15e05dafbeaeaf","b806dd4b2dce4d67b8bcede59ef937d5","b57d9e96db624f4eb43f2d9e9db01220","91a26d65822844b881cbd023ab824d88","7581707b4d7e4e5a833e9d89a0c2d66e","195d36c5a9344f67aff7ff96f0356ae2","2af49d6afc3e4fbfad7b1a0ab8257a4a","b7882fed4a0f4bfd9f0a6a6372367eda","ebaa283bdc21477a9b6801a9d8099fb7","31a1e0fe0602407a9042851f35116f57","0ca188a34d8e49b9980e98864d9a1013","383b400ac6d148308d846541dee345a6","e145b355d324431fa4349adddf56b4d7","5e262d97a0674ca9a42e9d23ddaee3c5","ca690430311a4f08949a15c5996cc4c7","cc764acda441431d9d1bc28394b50942","f35169ae6692462e973ba78bdc17e4d8","577c5fd437f54a959d26789b79659a4d","2493e81822cc4b9788be188f1e0611d8","d6f143928c9040dfabf0a7a04a494fbf","2f42361fdd2b45cfabcb4cdcdaf71eae","c31fa137ae9a431098d53a04818d4680","448a64809fef4d7db608673da60afd7d","f16e3dc4c4f04671b107d27644362932","d50dad19b3c54129bbb63082059b4986","b09d6324b1f047848eba4a88b3cddcf9","007b93cae166449d9480d678619e4010","805e969ea6b64dc88233c240dc410656","a5a5fdd1b6044af8be34cc47a2e69dd0"]},"outputId":"d2080750-d2aa-402e-cc71-e14f2bfb7227"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea97689d8f824f7a947b70751b93dbb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1d069a3c2a48ab94981d7a01b27bc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7882fed4a0f4bfd9f0a6a6372367eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2493e81822cc4b9788be188f1e0611d8"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Part 2: Training a Small-Size BERT Model\n\nIn this part, we will explore how to build and train a small-sized BERT model for our classification task. Instead of using the full-sized BERT model, which is computationally expensive, we will create a smaller version using the Transformers library.","metadata":{"id":"aMcc2gsbt0iJ"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# 1. Define your BERT model for sequence classification\n#    Ensure that you set up the configuration properly (e.g., specify the number of output labels).\n# 2. Print the total number of trainable parameters in the model to understand its size.\n\n######################  TODO  ########################\n######################  TODO  ########################\n\n# Define BERT model for sequence classification\n# config = BertConfig(\n#     hidden_size=128,\n#     num_hidden_layers=2,\n#     num_attention_heads=4,\n#     intermediate_size=512,\n#     num_labels=len(label_encoder.classes_)\n# )\n# model = BertForSequenceClassification(config)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\nprint(\"Number of trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))","metadata":{"id":"3RS5oBz3qmvu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e98472ac-6c4e-4414-a250-c980ec34f2a3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Number of trainable parameters: 109486085\n"]}],"execution_count":15},{"cell_type":"markdown","source":"---\n\nNow that you have defined your model, it's time to train it!☠️\n\nTraining a model of this size can take some time, depending on the available resources. To manage this, you can train your model for just **2–3 epochs** to demonstrate progress. Here are some hints:\n- **Training Metrics:** Ensure you print enough metrics, such as loss and accuracy, to track the training progress.\n- **Interactive Monitoring:** Use the `tqdm` library to display the progress of your training loop in real-time.","metadata":{"id":"Xr4Z14a6wL2c"}},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# optimizer = # TODO\n# num_epochs = # TODO\n\n# # Training loop\n# for epoch in range(num_epochs):\n\n#     model.train()\n\n#     for batch in tqdm(train_loader):\n#         optimizer.zero_grad()\n\n#         input_ids = # TODO\n#         attention_mask = # TODO\n#         labels = # TODO\n\n#         outputs = model(\n#             input_ids=input_ids,\n#             attention_mask=attention_mask,\n#             labels=labels\n#         )\n\n#         # TODO: Perform backpropagation and update the optimizer. Hint: Use outputs.loss to access the model's loss.\n\n#         # TODO: Monitor the training process by reporting metrics such as loss and accuracy.\n\n\n# # TODO : Evaluate the model on test dataset\n\n######################  TODO  ########################\n######################  TODO  ########################\n# Set up optimizer and training loop\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 3\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs.logits, dim=1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}, Average Train Loss: {avg_loss:.4f}, Train Accuracy: {100 * correct / total:.2f}%\")\n\n    # Evaluation\n    model.eval()\n    total_val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        # for batch in test_loader:\n        for batch in tqdm(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            loss = outputs.loss\n            total_val_loss += loss.item()\n            _, predicted = torch.max(outputs.logits, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = total_val_loss / len(test_loader)\n    print(f\"Epoch {epoch + 1}, Average Val Loss: {avg_val_loss:.4f}, Val Accuracy: {100 * correct / total:.2f}%\")\n\n# TODO : Evaluate the model on test dataset\nprint(\"Evaluating the model on test dataset\")\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        _, predicted = torch.max(outputs.logits, dim=1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"id":"FRZW-F9Dw6AI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e66470d-5652-4ce0-ee28-fe27a948c358"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 205/205 [10:40<00:00,  3.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Average Train Loss: 1.0349, Train Accuracy: 54.10%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:58<00:00,  1.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Average Val Loss: 0.7287, Val Accuracy: 69.82%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 205/205 [10:40<00:00,  3.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Average Train Loss: 0.6567, Train Accuracy: 72.95%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:58<00:00,  1.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Average Val Loss: 0.6361, Val Accuracy: 73.61%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 205/205 [10:39<00:00,  3.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Average Train Loss: 0.5152, Train Accuracy: 79.38%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:58<00:00,  1.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Average Val Loss: 0.6600, Val Accuracy: 74.22%\n","Evaluating the model on test dataset\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:58<00:00,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7422\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"execution_count":16},{"cell_type":"markdown","source":"## Part 3: Fine-Tuning TinyBERT with LoRA\n\nAs you have experienced, training even a small-sized BERT model can be computationally intensive and time-consuming. To address these challenges, we explore **Parameter-Efficient Fine-Tuning (PEFT)** methods, which allow us to utilize the power of large pretrained models without requiring extensive resources.\n\n---\n\n### **Parameter-Efficient Fine-Tuning (PEFT)**\n\nPEFT methods focus on fine-tuning only a small portion of the model’s parameters while keeping most of the pretrained weights frozen. This drastically reduces the computational and storage requirements while leveraging the rich knowledge embedded in pretrained models.\n\nOne popular PEFT method is LoRA (Low-Rank Adaptation).\n\n- **What is LoRA?**\n\nLoRA introduces a mechanism to fine-tune large language models by injecting small low-rank matrices into the model's architecture. Instead of updating all parameters during training, LoRA trains these small matrices while keeping the majority of the original parameters frozen.  This is achieved as follows:\n\n1. **Frozen Weights**: The pretrained weights of the model, represented as a weight matrix $ W \\in \\mathbb{R}^{d \\times k} $, remain **frozen** during fine-tuning.\n\n2. **Low-Rank Decomposition**:\n   Instead of directly updating $ W $, LoRA introduces two trainable matrices, $ A \\in \\mathbb{R}^{d \\times r} $ and $ B \\in \\mathbb{R}^{r \\times k} $, where $ r \\ll \\min(d, k) $.  \n   These matrices approximate the update to $ W $ as:\n   $$\n   \\Delta W = A \\cdot B\n   $$\n\n   Here, $ r $, the rank of the decomposition, is a key hyperparameter that determines the trade-off between computational cost and model capacity.\n\n3. **Adaptation**:\n   During training, instead of updating $ W $, the adapted weight is:\n   $$\n   W' = W + \\Delta W = W + A \\cdot B\n   $$\n   Only the low-rank matrices $ A $ and $ B $ are optimized, while $ W $ remains fixed.\n\n4. **Efficiency**:\n   Since $ r $ is much smaller than $ d $ and $ k $, the number of trainable parameters in $ A $ and $ B $ is significantly less than in $ W $. This makes the approach highly efficient both in terms of computation and memory.\n\n---\n\n###  **Fine-Tuning TinyBERT**\n\nFor this part, we will fine-tune **TinyBERT**, a distilled version of BERT, using the LoRA method.\n\n- **What is TinyBERT?**\n\nTinyBERT is a lightweight version of the original BERT model created through knowledge distillation. It significantly reduces the model size and inference latency while preserving much of the original BERT’s effectiveness. Here are some key characteristics of TinyBERT:\n- It is designed to be more resource-efficient for tasks such as classification, question answering, and more.\n- TinyBERT retains a compact structure with fewer layers and parameters, making it ideal for fine-tuning with limited computational resources.\n","metadata":{"id":"-yHtTYcpz6AW"}},{"cell_type":"markdown","source":"> Similar to the previous section, training this model might take some time. Given the resource limitations, you can train the model for just **2-3 epochs** to demonstrate the process.\n","metadata":{"id":"n_Og-pBeV5x6"}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"id":"fe1vGCZwU7MZ"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# # Load the pre-trained TinyBERT\n# model_name = \"prajjwal1/bert-tiny\"\n# base_model = # TODO\n# tokenizer = # TODO\n\n# # Define LoRA Configuration\n# lora_config = # TODO\n\n######################  TODO  ########################\n######################  TODO  ########################\n# Load the pre-trained TinyBERT\nmodel_name = \"prajjwal1/bert-tiny\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Define LoRA Configuration\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=8,  # Rank of the low-rank matrices\n    lora_alpha=16,  # Scaling factor\n    lora_dropout=0.1,  # Dropout rate\n    # target_modules=[\"query\", \"key\", \"value\"], # Modules to apply LoRA\n    target_modules=[\"query\", \"value\"],\n)","metadata":{"id":"LIyN5vOLLWz6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a683c61-1252-497a-c043-7ddc7e6936d5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"execution_count":27},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# # Apply LoRA to model\n# lora_model = # TODO\n\n# # TODO: Show the number of trainable parameters\n\n# # Training configuration\n# optimizer = # TODO\n# criterion = # TODO\n\n######################  TODO  ########################\n######################  TODO  ########################\n# Apply LoRA to model\nlora_model = get_peft_model(base_model, lora_config)\n\n# Show the number of trainable parameters\ntrainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params}\")\n\n# Training configuration\noptimizer = AdamW(lora_model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()","metadata":{"id":"jMgwZ8YmLuZ_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3675413d-6295-43b1-a50c-c29cce623593"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trainable parameters: 8837\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"execution_count":28},{"cell_type":"code","source":"######################  TODO  ########################\n######################  TODO  ########################\n\n# optimizer = # TODO\n# num_epochs = # TODO\n\n# # Training loop\n# for epoch in range(num_epochs):\n\n#     lora_model.train()\n\n#     for batch in tqdm(train_loader):\n#         optimizer.zero_grad()\n\n#         input_ids = # TODO\n#         attention_mask = # TODO\n#         labels = # TODO\n\n#         outputs = lora_model(\n#             input_ids=input_ids,\n#             attention_mask=attention_mask,\n#             labels=labels\n#         )\n\n#         # TODO: Perform backpropagation and update the optimizer. Hint: Use outputs.loss to access the model's loss.\n\n#         # TODO: Monitor the training process by reporting metrics such as loss and accuracy.\n\n\n# # TODO : Evaluate the model on test dataset\n\n######################  TODO  ########################\n######################  TODO  ########################\n\noptimizer = AdamW(lora_model.parameters(), lr=1e-3)\nnum_epochs = 5\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlora_model.to(device)\n# Training loop\nfor epoch in range(num_epochs):\n    lora_model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = lora_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        _, predicted = torch.max(outputs.logits, dim=1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}, Average Train Loss: {avg_loss:.4f}, Train Accuracy: {100 * correct / total:.2f}%\")\n\n    # Evaluation\n    lora_model.eval()\n    total_val_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        # for batch in test_loader:\n        for batch in tqdm(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = lora_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels  # Add labels here\n            )\n\n            loss = outputs.loss\n            total_val_loss += loss.item()\n            _, predicted = torch.max(outputs.logits, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = total_val_loss / len(test_loader)\n    print(f\"Epoch {epoch + 1}, Average Val Loss: {avg_val_loss:.4f}, Val Accuracy: {100 * correct / total:.2f}%\")\n\nprint(\"Evaluating the model on test dataset\")\n# TODO : Evaluate the model on test dataset\nlora_model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = lora_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        _, predicted = torch.max(outputs.logits, dim=1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"id":"J395FrcWMbmx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98efa99c-86cf-4499-f12e-7de6172a9b17"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","100%|██████████| 205/205 [00:48<00:00,  4.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Average Train Loss: 1.1119, Train Accuracy: 48.04%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:07<00:00,  7.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Average Val Loss: 0.9871, Val Accuracy: 52.72%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 205/205 [00:43<00:00,  4.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Average Train Loss: 0.9409, Train Accuracy: 57.17%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:07<00:00,  6.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Average Val Loss: 0.8596, Val Accuracy: 60.17%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 205/205 [00:43<00:00,  4.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Average Train Loss: 0.8771, Train Accuracy: 60.75%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:07<00:00,  6.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Average Val Loss: 0.8379, Val Accuracy: 62.80%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 205/205 [00:49<00:00,  4.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Average Train Loss: 0.8414, Train Accuracy: 62.75%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:07<00:00,  6.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Average Val Loss: 0.8143, Val Accuracy: 63.71%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 205/205 [00:50<00:00,  4.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Average Train Loss: 0.8142, Train Accuracy: 63.57%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:07<00:00,  6.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Average Val Loss: 0.8061, Val Accuracy: 65.79%\n","Evaluating the model on test dataset\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:09<00:00,  5.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.6579\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"id":"s79gx6SPHJBk"},"outputs":[],"execution_count":null}]}