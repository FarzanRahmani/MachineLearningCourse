{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b06e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1Ua3QAi0oNMwM4-bmygY-HTDHx_ApwZZW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f1fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfac4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip SharifML_Contest_NLP.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b07bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d58ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_csv('labels.csv')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = VQADataset(labels_df, 'animals', processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ca820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_csv('lables.csv')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = VQADataset(labels_df, 'animals', processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3959e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7470f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test info\n",
    "test_df = pd.read_csv('test_info.csv')\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = VQADataset(test_df, 'test_images', processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Prediction loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(predicted_ids.cpu().numpy())\n",
    "\n",
    "# Update the test dataframe with predictions\n",
    "test_df['answer'] = predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = test_df[['file_name', 'question', 'answer']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b7033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ValVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        # answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Load the test info\n",
    "test_df = pd.read_csv('test_info.csv')\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = ValVQADataset(test_df, 'test_images', processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Prediction loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(predicted_ids.cpu().numpy())\n",
    "\n",
    "# Update the test dataframe with predictions\n",
    "test_df['answer'] = predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = test_df[['file_name', 'question', 'answer']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e717760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'SharifML_Contest_NLP.ipynb')):\n",
    "    %notebook -e SharifML_Contest_NLP.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "file_names = ['SharifML_Contest_NLP.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4fd3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_csv('lables.csv')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "# train_dataset = VQADataset(labels_df, 'animals', processor)\n",
    "train_dataset = VQADataset(labels_df, 'test_images', processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c60befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5e1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_csv('lables.csv')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "# train_dataset = VQADataset(labels_df, 'animals', processor)\n",
    "train_dataset = VQADataset(labels_df, 'test_images', processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e10f4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c17f1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_csv('lables.csv')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        encoding['labels'] = torch.tensor(int(answer), dtype=torch.float)  # Use float for regression\n",
    "        return encoding\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Modify the model's output layer for regression\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 1)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "# train_dataset = VQADataset(labels_df, 'animals', processor)\n",
    "train_dataset = VQADataset(labels_df, 'test_images', processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d739cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the labels\n",
    "labels_df = pd.read_csv('lables.csv')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        encoding['labels'] = torch.tensor(int(answer), dtype=torch.float)  # Use float for regression\n",
    "        return encoding\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Modify the model's output layer for regression\n",
    "# model.classifier = torch.nn.Linear(model.classifier.in_features, 1)\n",
    "model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, 1)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "# train_dataset = VQADataset(labels_df, 'animals', processor)\n",
    "train_dataset = VQADataset(labels_df, 'test_images', processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddfc4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, labels=labels)\n",
    "        # loss = outputs.loss\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        logits = outputs.logits.squeeze(-1)\n",
    "        loss = nn.MSELoss()(logits, labels)  # Use MSE loss for regression\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c118486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, labels=labels)\n",
    "        # loss = outputs.loss\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        logits = outputs.logits.squeeze(-1)\n",
    "        loss = nn.MSELoss()(logits, labels)  # Use MSE loss for regression\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "283d443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ValVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        # answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Load the test info\n",
    "test_df = pd.read_csv('test_info.csv')\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = ValVQADataset(test_df, 'test_images', processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Prediction loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(predicted_ids.cpu().numpy())\n",
    "\n",
    "# Update the test dataframe with predictions\n",
    "test_df['answer'] = predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = test_df[['file_name', 'question', 'answer']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72c31b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict and save the predictions\n",
    "# # TODO\n",
    "# # submission =\n",
    "# def predict(test_csv_path):\n",
    "#     test_df = pd.read_csv(test_csv_path)\n",
    "#     submission = []\n",
    "    \n",
    "#     for _, row in test_df.iterrows():\n",
    "#         image = Image.open(os.path.join('test_images', row['file_name']))\n",
    "#         question = row['question']\n",
    "        \n",
    "#         encoding = processor(\n",
    "#             image, question, \n",
    "#             return_tensors=\"pt\", \n",
    "#             padding=\"max_length\", \n",
    "#             truncation=True\n",
    "#         ).to(device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**encoding)\n",
    "        \n",
    "#         pred = torch.argmax(outputs.logits).item() + 1  # Convert back to 1-4\n",
    "#         submission.append({\n",
    "#             'file_name': row['file_name'],\n",
    "#             'question': question,\n",
    "#             'answer': pred\n",
    "#         })\n",
    "    \n",
    "#     return pd.DataFrame(submission)\n",
    "\n",
    "# # Generate submission\n",
    "# submission = predict('test_info.csv')\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# ****************\n",
    "\n",
    "# Prepare the submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'file_name': test_df['file_name'],\n",
    "    'question': test_df['question'],\n",
    "    'answer': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission dataframe to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "483588d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ValVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        # answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Load the test info\n",
    "test_df = pd.read_csv('test_info.csv')\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = ValVQADataset(test_df, 'test_images', processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Prediction loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(predicted_ids.cpu().numpy())\n",
    "\n",
    "# Update the test dataframe with predictions\n",
    "test_df['answer'] = predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = test_df[['file_name', 'question', 'answer']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cff6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'SharifML_Contest_NLP.ipynb')):\n",
    "    %notebook -e SharifML_Contest_NLP.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "file_names = ['SharifML_Contest_NLP.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acf5ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ValVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        # answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Load the test info\n",
    "test_df = pd.read_csv('test_info.csv')\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = ValVQADataset(test_df, 'test_images', processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Prediction loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        print(outputs)\n",
    "        logits = outputs.logits\n",
    "        print(logits)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(predicted_ids.cpu().numpy())\n",
    "        break\n",
    "\n",
    "# Update the test dataframe with predictions\n",
    "test_df['answer'] = predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = test_df[['file_name', 'question', 'answer']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3c6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        print(labels)\n",
    "\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, labels=labels)\n",
    "        # loss = outputs.loss\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        print(outputs)\n",
    "        logits = outputs.logits.squeeze(-1)\n",
    "        print(logits)\n",
    "        loss = nn.MSELoss()(logits, labels)  # Use MSE loss for regression\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad0a0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ValVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        # answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Load the test info\n",
    "test_df = pd.read_csv('test_info.csv')\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = ValVQADataset(test_df, 'test_images', processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Prediction loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        print(outputs)\n",
    "        logits = outputs.logits\n",
    "        print(logits)\n",
    "        # predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predicted_ids = torch.round(logits).int()\n",
    "        print(predicted_ids)\n",
    "\n",
    "        predictions.extend(predicted_ids.cpu().numpy())\n",
    "        break\n",
    "\n",
    "# Update the test dataframe with predictions\n",
    "test_df['answer'] = predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = test_df[['file_name', 'question', 'answer']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdbad6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class ValVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        question = item['question']\n",
    "        # answer = item['answer']\n",
    "        encoding = self.processor(images=image, text=question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # encoding['labels'] = torch.tensor(int(answer), dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "# Load the test info\n",
    "test_df = pd.read_csv('test_info.csv')\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = ValVQADataset(test_df, 'test_images', processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Prediction loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
    "        pixel_values = batch['pixel_values'].squeeze(1).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        # print(outputs)\n",
    "        logits = outputs.logits\n",
    "        # print(logits)\n",
    "        # predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        predicted_ids = torch.round(logits).int()\n",
    "        # print(predicted_ids)\n",
    "\n",
    "        predictions.extend(predicted_ids.cpu().numpy())\n",
    "\n",
    "# Update the test dataframe with predictions\n",
    "test_df['answer'] = predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = test_df[['file_name', 'question', 'answer']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
